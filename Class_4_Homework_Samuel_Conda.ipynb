{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "This week marks a major shift in your AI agent's capabilities: you’ll build the foundation for a Retrieval-Augmented Generation (RAG) system tailored to scientific research. Rather than relying on an LLM’s memory alone, RAG architectures allow your agent to search a structured knowledge base and generate grounded, document-aware answers.\n",
    "\n",
    "Your task is to create a RAG pipeline using recent arXiv cs.CL papers, converting them into searchable chunks, embedding them, and indexing them with FAISS. You’ll then implement a simple query interface that takes a user question, retrieves the top relevant chunks, and displays them for further processing.\n",
    "\n",
    "This week marks the beginning of building your agent’s private research knowledge base—a semantic index that you’ll evolve into a full-featured hybrid database in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 📚 Learning Objectives\n",
    "\n",
    "* Understand the components of a Retriever-Reader QA pipeline.\n",
    "* Explore document chunking strategies (e.g., sections vs. sliding windows) and their impact on retrieval performance.\n",
    "* Index scientific text using vector embeddings and FAISS.\n",
    "* Build and query a semantic index via a FastAPI endpoint that returns relevant passages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create rag_env virtual environment\n",
    "\n",
    "(base) C:\\Users\\ch939>conda create -n rag_env python=3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the vitual environment\n",
    "\n",
    "(base) C:\\Users\\ch939>conda activate rag_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation of sentence-transformers is time consuming, 10 minutes? \n",
    "(rag_env) C:\\Users\\ch939>conda install sentence-transformers\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install arxiv\n",
    "\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install fastapi\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install uvicorn\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install nest-asyncio\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install jupyter\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install numpy\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install tqdm\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install -c pytorch faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install FAISS-GPU. \n",
    "\n",
    "The installations works in Rag-env using the below command, not in this jupyter,\n",
    "\n",
    "conda install -c pytorch faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Anaconda Prompt, create Python environment with Python 3.10 and activate the environmwent\n",
    "(base) C:\\Users\\ch939>conda create -n rag_env python=3.10\n",
    "(base) C:\\Users\\ch939>conda activate rag_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Pytorch in the rag_env virtual environment:\n",
    "(rag_env) C:\\Users\\ch939>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install other packages:\n",
    "(rag_env) C:\\Users\\ch939>conda install arxiv\n",
    "(rag_env) C:\\Users\\ch939>conda install fastapi\n",
    "(rag_env) C:\\Users\\ch939>conda install uvicorn\n",
    "(rag_env) C:\\Users\\ch939>conda install nest-asyncio\n",
    "(rag_env) C:\\Users\\ch939>conda install jupyter\n",
    "(rag_env) C:\\Users\\ch939>conda install numpy\n",
    "(rag_env) C:\\Users\\ch939>conda install tqdm\n",
    "(rag_env) C:\\Users\\ch939>conda install -c pytorch faiss-gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries installed in the implemetation of the below steps:\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install -c conda-forge pymupdf // not working\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>pip install pymupdf\n",
    "\n",
    "(rag_env) C:\\Users\\ch939>conda install -c conda-forge pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using \"ctl+shft+p\" to switch the kernel of jupyter in VS code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod4env.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Optional: Validate that the key is loaded\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set it in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Python Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 12.8\n",
      "Device Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Device Count:\", torch.cuda.device_count())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify FAISS sees GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(\"FAISS GPU Available:\", hasattr(faiss, \"GpuIndexFlatL2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ch939\\anaconda3\\envs\\rag_env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "list_of_chunks = [\n",
    "    \"This is the first text chunk.\",\n",
    "    \"Here is the second text chunk, which is a bit longer than the first one.\",\n",
    "    \"Finally, this is the third text chunk, and it contains some more information.\"\n",
    "]\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2',use_auth_token=False)\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (Alternatively, you can use a Hugging Face Transformer model and apply pooling manually to get chunk embeddings.)\n",
    "   \n",
    "5. **Indexing with FAISS:** Build a FAISS index of the chunk embeddings. For example, use a simple index like `IndexFlatL2` with the same dimensionality as your embeddings. Add all chunk vectors to the index (e.g., `index.add(np.array(embeddings))`).\n",
    "6. **Notebook Demo:** Create a notebook where a user query is embedded and passed to the index (`index.search(query_embedding, k)`) to retrieve the top-3 matching chunks. Display the original chunk text for these results.\n",
    "7. **FastAPI Service:** Build a simple FastAPI app. Define an endpoint (e.g. `@app.get(\"/search\")`) that accepts a query parameter `q`. In the handler, embed `q`, perform the FAISS search, and return the top passages as JSON. (For example, a FastAPI endpoint can accept a question and return relevant documents.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Below are skeleton code templates. Fill in the details (indicated by comments or ellipses).\n",
    "\n",
    "**Download arxiV Papers (cs.CL):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:cs.CL\",\n",
    "    max_results=50,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "papers = []\n",
    "for result in client.results(search):\n",
    "    result.download_pdf(dirpath=\"data/pdfs/\", filename=f\"{result.entry_id.split('/')[-1]}.pdf\")\n",
    "    papers.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Extraction (PDF → Text):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if fits has been downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF Version: 1.26.3\n",
      "PyMuPDF Docstring:\n",
      "PyMuPDF 1.26.3: Python bindings for the MuPDF 1.26.3 library (rebased implementation).\n",
      "Python 3.10 running on win32 (64-bit).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fitz # PyMuPDF\n",
    "print(\"PyMuPDF Version:\", fitz.__version__)\n",
    "print(\"PyMuPDF Docstring:\")\n",
    "print(fitz.__doc__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract_text_from_pdf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative extract_text_from_pdf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    from PyPDF2 import PdfReader  # Import inside function or globally\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:  # Avoid adding None\n",
    "            text += page_text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try one pdf first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphical Abstract\n",
      "Decoding Neural Emotion Patterns through Natural Language Pro-\n",
      "cessing Embeddings\n",
      "Gideon Vos, Maryam Ebrahimpour, Liza van Eijk, Zoltan Sarnyai, Mostafa\n",
      "Rahimi Azghadi\n",
      "arXiv:2508.09337v1  [cs.CL]  12 Aug 2025\n",
      "\n",
      " \n",
      "2\n",
      "\n",
      "Highlights\n",
      "Decoding Neural Emotion Patterns through Natural Language Pro-\n",
      "cessing Embeddings\n",
      "Gideon Vos, Maryam Ebrahimpour, Liza van Eijk, Zoltan Sarnyai, Mostafa\n",
      "Rahimi Azghadi\n",
      "• This study introduces a computational framework for directly mapping\n",
      "natural language emotional content to brain regions without requiring\n",
      "neuroimaging.\n",
      "• The integration of semantic embeddings and neuro-anatomical map-\n",
      "ping successfully differentiated between healthy and depressed popula-\n",
      "tions through distinct limbic activation patterns.\n",
      "• The framework demonstrated high spatial specificity by accurately map-\n",
      "ping twenty-seven discrete emotions to neuro-anatomically plausible\n",
      "brain regions.\n",
      "• Regional assignment patterns showed strong consistency with estab-\n",
      "lished neuroimaging research.\n",
      "• In favor of reproducible research and to advance the field, all program-\n",
      "ming code used in this study is made publicly available.\n",
      "\n",
      "Decoding Neural Emotion Patterns through Natural\n",
      "Language Processing Embeddings\n",
      "Gideon Vosa, Maryam Ebrahimpoura, Liza van Eijkb, Zoltan Sarnyaic,\n",
      "Mostafa Rahimi Azghadia\n",
      "aCollege of Science and Engineering, James Cook University, James Cook\n",
      "Dr, Townsville, 4811, QLD, Australia\n",
      "bCollege of Health Care Sciences, James Cook University, James Cook\n",
      "Dr, Townsville, 4811, QLD, Australia\n",
      "cCollege of Public Health, Medical, and Vet Sciences, James Cook University, James\n",
      "Cook Dr, Townsville, 4811, QLD, Australia\n",
      "Abstract\n",
      "Introduction. Understanding the neural correlates of emotional expression\n",
      "in natural language represents a significant challenge in computational neu-\n",
      "roscience and affective computing. While traditional neuroimaging studies\n",
      "require expensive equipment and controlled laboratory environments, the\n",
      "increasing availability of digital text data presents new opportunities for\n",
      "emotion-brain mapping. Previous research has primarily focused on either\n",
      "neuroimaging-based emotion localization or computational text analysis in-\n",
      "dependently, with limited integration between these domains. This study\n",
      "proposes a novel computational approach that, while not validated against\n",
      "direct neuroimaging, explores potential relationships between textual emo-\n",
      "tional content and anatomically-defined brain regions.\n",
      "Methods. We developed a computational pipeline that maps textual emo-\n",
      "tion expressions to specific brain regions through a multi-stage process. The\n",
      "framework utilizes OpenAI’s text-embedding-ada-002 model to generate high-\n",
      "dimensional semantic representations of input texts, followed by dimensional-\n",
      "ity reduction and clustering to identify emotional clusters. These clusters are\n",
      "then mapped to 18 predefined neuro-anatomic brain regions associated with\n",
      "emotional processing.\n",
      "Three distinct experiments were conducted.\n",
      "First,\n",
      "we analyzed conversational data from healthy and depressed subjects using\n",
      "the Distress Analysis Interview Corpus/Wizard-of-Oz (DIAC-WOZ) dataset,\n",
      "comparing emotional brain mapping patterns between these populations.\n",
      "Preprint submitted to Computers in Biology and Medicine\n",
      "August 14, 2025\n",
      "\n",
      "Next, we repeated this process the comprehensive GoEmotions classification\n",
      "dataset. Emotional intensity was quantified using a lexical scoring system\n",
      "that evaluates keyword presence, syntactic patterns, and linguistic modifiers.\n",
      "Finally, we performed a comparison between human-produced text and the\n",
      "response generated by a Large Language Model (LLM) chat bot to evaluate\n",
      "differences in emotional brain activation patterns and determine the extent\n",
      "to which AI-generated language mirrors human emotional expression.\n",
      "Results. Our proposed approach successfully mapped textual emotions to\n",
      "neuro-anatomically plausible brain regions with high spatial specificity across\n",
      "experiments. Distinct activation patterns emerged between healthy and de-\n",
      "pressed populations, with depressed subjects showing increased engagement\n",
      "of limbic regions associated with negative affect processing. Extended emo-\n",
      "tion analysis demonstrated successful differentiation of discrete emotional\n",
      "states. Our final experimental results revealed that while LLM-generated\n",
      "responses exhibited a similar distribution of basic emotions, they lacked the\n",
      "nuanced regional activation observed in human text, particularly in areas\n",
      "associated with empathy and self-referential processing such as the medial\n",
      "prefrontal cortex and posterior cingulate cortex.\n",
      "Conclusion. This study presents a computational framework for directly\n",
      "mapping natural language emotional content to brain regions without re-\n",
      "quiring neuroimaging data. The novel integration of semantic embeddings,\n",
      "unsupervised clustering, and neuro-anatomical mapping provides a scalable\n",
      "approach for emotion-brain research that can process large-scale textual\n",
      "datasets. The framework’s ability to distinguish between healthy and de-\n",
      "pressed populations, as well as differentiate among discrete emotions, demon-\n",
      "strates its potential for both clinical applications and basic emotion research.\n",
      "The methodology offers significant advantages over traditional neuroimaging\n",
      "approaches, including cost-effectiveness, scalability, and the ability to analyze\n",
      "naturalistic language data. Finally, this framework provides a brain-inspired\n",
      "benchmark for evaluating how closely AI-generated language mirrors human\n",
      "emotional expression by comparing their inferred neural activation patterns.\n",
      "Keywords:\n",
      "Artificial Intelligence, Mental Health, Depression\n",
      "PACS: 07.05.Mh, 87.19.La\n",
      "2000 MSC: 68T01, 92-08\n",
      "2\n",
      "\n",
      "1. Introduction\n",
      "Understanding the neural correlates of emotion has traditionally relied on\n",
      "neuroimaging modalities such as electroencephalography (EEG) and func-\n",
      "tional magnetic resonance imaging (fMRI) [1–3]. These techniques have re-\n",
      "vealed the involvement of regions like the amygdala, insula, anterior cingu-\n",
      "late cortex, and prefrontal cortex across different emotional states [4]. Meta-\n",
      "analyses of neuroimaging studies have consistently identified these key regions\n",
      "across diverse emotional paradigms, with the amygdala showing particular\n",
      "importance for threat detection and fear processing [5], while the anterior\n",
      "cingulate cortex and insula demonstrate critical roles in emotional awareness\n",
      "and interoceptive processing [6, 7]. However, traditional neuroimaging ap-\n",
      "proaches face significant limitations including high costs, restricted accessibil-\n",
      "ity, controlled laboratory requirements, and limited ecological validity when\n",
      "studying naturalistic emotional expression [8, 9]. The increasing availability\n",
      "of digital text data presents unprecedented opportunities for emotion-brain\n",
      "mapping that could overcome these existing constraints.\n",
      "Caucheteux et al. [10] demonstrated that large language model (LLM) em-\n",
      "beddings align with human brain activity without fine-tuning, showing that\n",
      "pre-trained language models inherently capture aspects of neural language\n",
      "representations. This foundational work was extended by Toneva et al. [11],\n",
      "who introduced the concept of brain embeddings, highlighting the geometric\n",
      "alignment between the representational spaces of LLMs and brain activity\n",
      "during reading and listening tasks.\n",
      "Further evidence of this alignment comes from Schrimpf et al.\n",
      "[12], who\n",
      "showed that artificial neural networks, especially transformer-based architec-\n",
      "tures as used in LLMs, can predict human neural responses to language with\n",
      "remarkable accuracy. These findings collectively support the feasibility of\n",
      "leveraging LLM-derived embeddings to model brain activity, suggesting that\n",
      "the semantic representations learned by these models may reflect fundamen-\n",
      "tal aspects of how the human brain processes language and emotion.\n",
      "Parallel efforts have explored mapping emotional text representations to\n",
      "neuro-biological substrates.\n",
      "Tomasino et al.\n",
      "[13] developed a cognitive-\n",
      "affective framework demonstrating how emotionally charged linguistic input\n",
      "recruits distinct neural systems, while Chen et al. [14] correlated sentiment\n",
      "3\n",
      "\n",
      "analysis outputs with fMRI patterns, confirming the brain’s differentiation of\n",
      "emotional valence during narrative comprehension. These studies build upon\n",
      "earlier work demonstrating that emotional processing involves distributed\n",
      "neural networks, with positive emotions preferentially engaging left prefrontal\n",
      "regions and negative emotions showing stronger right hemisphere activation\n",
      "[15].\n",
      "Zhou et al.\n",
      "[16] extended this work by associating semantic embeddings\n",
      "from emotional narratives with fMRI-derived brain states, particularly not-\n",
      "ing strong alignment in medial prefrontal and temporal regions. Similarly,\n",
      "Xiao et al. [17] applied unsupervised clustering on emotion-labeled text to\n",
      "uncover latent emotional dimensions and correlated them with EEG and\n",
      "fMRI features.\n",
      "These computational approaches align with neuroimaging\n",
      "meta-analyses showing that different emotional categories activate distinct\n",
      "but overlapping brain networks, with cognitive emotions recruiting prefrontal\n",
      "cortical areas [18, 19].\n",
      "The utility of natural text has been further emphasized by Hoemann et al.\n",
      "[20], who highlighted the value of social media and dialogue data for studying\n",
      "emotion in real-world contexts. Their findings support using large-scale spon-\n",
      "taneous language datasets to infer affective brain states, moving beyond the\n",
      "artificial constraints of laboratory-based emotion elicitation paradigms. This\n",
      "approach is particularly relevant given further research showing that nat-\n",
      "uralistic emotional expression differs significantly from laboratory-induced\n",
      "emotions in both linguistic patterns and associated neural activity [20].\n",
      "Despite these advances, existing studies have not established a fully compu-\n",
      "tational, imaging-free method that directly links textual emotional content\n",
      "to specific neuro-anatomical regions. Current approaches typically require\n",
      "either controlled laboratory settings that limit validity, or a focus on general\n",
      "emotional dimensions rather than specific brain region mapping [8].\n",
      "This represents a significant gap in our ability to study emotional processing\n",
      "at scale. The motivation for developing a purely computational emotion-\n",
      "brain mapping framework therefor stems from several converging factors.\n",
      "First, the exponential growth of digital text data offers an unprecedented\n",
      "window into human emotional expression in naturalistic contexts [20]. Sec-\n",
      "ond, advances in semantic embedding technologies have created powerful\n",
      "4\n",
      "\n",
      "tools for capturing nuanced relationships between language, meaning, and\n",
      "affective content. Third, decades of neuroimaging research have established\n",
      "a robust understanding of the neuro-anatomical basis of emotion processing,\n",
      "with meta-analytic evidence highlighting key neural circuits involved in emo-\n",
      "tional regulation and expression [18, 21].\n",
      "The theoretical foundation of our proposed approach rests on the principle\n",
      "that emotional expression in language reflects underlying neural processes.\n",
      "This hypothesis is supported by evidence showing that the human brain\n",
      "encodes language through distributed, continuous representations [22–24].\n",
      "Recent studies demonstrated a direct alignment between high-dimensional\n",
      "language model embeddings used by LLMs and neural activation patterns\n",
      "in language-processing regions [23–25]. Such findings suggest that embed-\n",
      "ding spaces learned by LLMs may mirror the brain’s own semantic encoding\n",
      "mechanisms, indirectly capturing semantic and emotional features that align\n",
      "with patterns of neural activity.\n",
      "Furthermore, individuals with certain mental health conditions may exhibit\n",
      "distinct language patterns, including variations in word choice, emotional\n",
      "tone, and syntactic complexity that correlate with well-documented neuro-\n",
      "biological abnormalities [8, 26–31]. Extending this perspective, characteristic\n",
      "language patterns in clinical populations may manifest as distinct clusters\n",
      "within embedding space, reflecting altered cognitive and affective process-\n",
      "ing [30, 32–34]).\n",
      "If these deviations also correspond to measurable shifts\n",
      "in brain activation, embedding-based models could offer novel insights into\n",
      "brain-language relationships and help inform computational mental health\n",
      "diagnostics.\n",
      "Beyond clinical contexts, this framework could further support advanced ap-\n",
      "plications in areas such as LLM-generated text detection [35], where subtle\n",
      "differences in linguistic patterns, coherence, and semantic clustering could\n",
      "indicate non-human authorship.\n",
      "This study therefor investigates whether it is possible to computationally\n",
      "map natural language emotional expressions directly to brain regions without\n",
      "neuroimaging data, using semantic embeddings and clustering techniques to\n",
      "bridge text analysis with neuro-anatomical mapping. Specifically, we aim to:\n",
      "5\n",
      "\n",
      "• Develop a novel computational framework that transforms textual emo-\n",
      "tional content into neuro-anatomically plausible brain region activa-\n",
      "tions using state-of-the-art natural language processing techniques.\n",
      "• Validate the clinical utility of this approach by demonstrating its abil-\n",
      "ity to differentiate between healthy and depressed populations through\n",
      "distinct emotional brain mapping patterns, building on established neu-\n",
      "roimaging differences in depression.\n",
      "• Demonstrate discrete emotion localization by mapping specific emo-\n",
      "tional states to neuro-anatomically appropriate brain regions with high\n",
      "spatial specificity, consistent with established emotion-brain mapping\n",
      "literature.\n",
      "• Provide an objective, brain-based measure of human-likeness in lan-\n",
      "guage by comparing AI-generated and human-authored texts through\n",
      "their inferred emotional brain activation patterns.\n",
      "By providing a cost-effective, scalable alternative to traditional neuroimag-\n",
      "ing methods, such a computational approach can open new avenues for un-\n",
      "derstanding the neural basis of human emotional communication in digital\n",
      "environments, while maintaining the ability to generate interpretable, neuro-\n",
      "anatomically grounded predictions from textual data alone.\n",
      "2. Methods\n",
      "2.1. Datasets, Preprocessing and Text Preparation\n",
      "Three text-based datasets were employed in this study (Table 1). The DIAC-\n",
      "WOZ dataset [36] comprises annotated interview transcripts from individuals\n",
      "diagnosed with depression and healthy controls. The GoEmotions dataset\n",
      "[37] includes 58,000 Reddit [38] comments manually labeled into 27 emotion\n",
      "categories (or neutral). The Schema-Guided Dialogue dataset [39] represents\n",
      "nearly half a million sentences comprised of human and LLM chat bot inter-\n",
      "actions. All datasets consist of texts produced by native English speakers.\n",
      "6\n",
      "\n",
      "Table 1: Datasets utilized in this study.\n",
      "Dataset\n",
      "Emotions\n",
      "Subjects\n",
      "DAICWOZ[36]\n",
      "Healthy and Depressed Categories\n",
      "134 Clinical interview transcripts\n",
      "GoEmotions [37]\n",
      "27 Emotion Categories\n",
      "58k English Reddit comments\n",
      "The Schema-Guided Dialogue Dataset [39]\n",
      "Human and Chat bot conversations\n",
      "463,282 English sentences\n",
      "2.2. Text Embedding Generation\n",
      "During preprocessing (Figure 1, step 1), text was segmented into chunks of\n",
      "approximately 300 characters using periods as sentence boundaries. Next,\n",
      "text embeddings were generated using OpenAIs [40] text-embedding-ada-002\n",
      "model (Figure 1, step 2), which produces 1536-dimensional vector represen-\n",
      "tations of input text.\n",
      "This model was chosen for its strong performance\n",
      "in capturing both semantic relationships and emotional nuances in natural\n",
      "language [28–31]. Although training a custom embeddings model for our ex-\n",
      "perimentation is technically feasible, it could introduce unwanted bias into\n",
      "our experiments, while the OpenAI text-embedding-ada-002 embeddings are\n",
      "commonly used in many public and commercial LLMs.\n",
      "7\n",
      "\n",
      "Figure 1: Five-step text computational pipeline to convert natural language text to embed-\n",
      "dings, reduce dimensionality, cluster to emotional groups and map to final brain regions.\n",
      "8\n",
      "\n",
      "2.3. Dimensionality Reduction and Spatial Mapping\n",
      "The high-dimensional embeddings underwent a dimensionality reduction pro-\n",
      "cess (Figure 1, step 3A) using Principal Component Analysis (PCA) to reduce\n",
      "the dimensionality to three components, representing the minimum number\n",
      "of dimensions required for spatial brain mapping.\n",
      "2.4. Emotional Intensity Estimation\n",
      "Emotional intensity was quantified using a lexicon-based approach [41–46]\n",
      "combined with syntactic feature analysis (Figure 1, step 3B). This emotion\n",
      "lexicon was constructed containing multiple terms across intensity levels:\n",
      "• Extreme intensity (1.0): Terms indicating maximum emotional activa-\n",
      "tion (e.g., ”devastated,” ”euphoric,” ”depressed,” ”extremely”)\n",
      "• High intensity (0.8):\n",
      "Strong emotional indicators (e.g., ”amazing,”\n",
      "”hate,” ”terrible”)\n",
      "• Moderate intensity (0.6): Common emotional expressions (e.g., ”love,”\n",
      "”sad,” ”happy”)\n",
      "• Mild intensity (0.3): Subtle emotional content (e.g., ”nice,” ”bad,”\n",
      "”okay”)\n",
      "Additional intensity modifiers were incorporated to capture amplification\n",
      "effects:\n",
      "• Intensification modifiers (”so,” ”very,” ”really,” ”truly,” ”completely,”\n",
      "”totally”) added 0.3 to base intensity\n",
      "• Absolutist terms (”never,” ”always,” ”everything,” ”nothing”) con-\n",
      "tributed 0.2 additional intensity\n",
      "• Exclamation marks added 0.25 per occurrence (maximum 4)\n",
      "• Question marks contributed 0.15 per occurrence (maximum 3)\n",
      "• All-caps text (more than 3 characters) added 0.5 intensity\n",
      "Base intensity was set at 0.1 for all texts to account for implicit emotional\n",
      "content, with final intensity scores capped at 2.0 to prevent extreme outliers\n",
      "from skewing subsequent analyses.\n",
      "9\n",
      "\n",
      "2.5. Emotion Region Clustering\n",
      "K-means clustering was applied to the 3D PCA-transformed embeddings to\n",
      "identify distinct emotional patterns within the data (Figure 1, step 4). The\n",
      "number of clusters was set to match the number of predefined anatomical\n",
      "brain regions (11 bilateral + 7 midline = 18 regions), establishing a direct cor-\n",
      "respondence between emotional content clusters and neuro-anatomical struc-\n",
      "tures [1, 5, 47–49]. Of the 18 anatomically defined brain regions selected, 14\n",
      "have been consistently implicated in emotion processing [1–3, 50].\n",
      "2.6. Cluster-to-Region Assignment\n",
      "The assignment of emotional content clusters to specific brain regions (Fig-\n",
      "ure 1, step 5) employed a two stage distance minimization approach that\n",
      "preserved the one-to-one mapping between clusters and anatomical regions.\n",
      "First, each cluster center’s coordinates in the 3D PCA space were compared\n",
      "against the predefined Montreal Neurological Institute (MNI) coordinate po-\n",
      "sitions of the 18 emotion processing brain regions using Euclidean distance.\n",
      "These regions were selected to span key nodes of the emotional circuitry and\n",
      "included limbic structures, prefrontal regions, subcortical structures, tempo-\n",
      "ral regions, and brain stem nuclei, along with anterior and posterior cingulate\n",
      "cortex and medial prefrontal cortex.\n",
      "The mapping process utilized a greedy assignment algorithm where cluster\n",
      "centers were sequentially matched to their nearest available brain regions.\n",
      "For each cluster center, distances to all anatomical regions were calculated\n",
      "and sorted in ascending order. The cluster was then assigned to the closest\n",
      "region that had not yet been claimed by another cluster, with this constraint\n",
      "preventing multiple clusters from mapping to the same anatomical location.\n",
      "This sequential assignment continued until all clusters were mapped to unique\n",
      "brain regions, ensuring that the spatial distribution of emotional content in\n",
      "the 3D space corresponded meaningfully to the anatomical organization of\n",
      "emotion-processing brain networks. Once cluster-to-region assignments were\n",
      "established, individual text samples inherited their brain region labels based\n",
      "on their cluster membership, creating the final mapping from textual content\n",
      "to neuro-anatomical locations.\n",
      "10\n",
      "\n",
      "2.7. Statistical Analysis\n",
      "The computational pipeline incorporated statistical practices including ran-\n",
      "dom seed setting to ensure reproducibility and management of edge cases\n",
      "such as insufficient sample sizes.\n",
      "Region-specific analysis was conducted\n",
      "by aggregating texts assigned to each brain region and calculating mean\n",
      "emotional intensities, providing quantitative measures of regional emotional\n",
      "activation patterns. This approach enabled between-group comparisons of\n",
      "emotion-brain mapping patterns. Figure 2 provides a visual hierarchy de-\n",
      "tailing the emotion to brain region mapping approach as detailed in steps 4\n",
      "and 5 of Figure 1.\n",
      "11\n",
      "\n",
      "Figure 2: Emotion to brain region assignment hierarchy applied in this study.\n",
      "12\n",
      "\n",
      "The programming pseudo-code for the three key algorithms utilized and de-\n",
      "scribed in Section 2 are detailed below:\n",
      "Algorithm 1 Text Preprocessing and Embedding Generation\n",
      "Input: Text datasets D = {d1, d2, d3}\n",
      "Output: 1536-dimensional embeddings matrix\n",
      "1: // Step 1: Text Preprocessing and Chunking\n",
      "2: function PreprocessTexts(texts)\n",
      "3:\n",
      "chunks ←[]\n",
      "4:\n",
      "for each text in texts do\n",
      "5:\n",
      "segments ←split text into ≈300 character chunks using periods\n",
      "6:\n",
      "chunks.append(segments)\n",
      "7:\n",
      "end for\n",
      "8:\n",
      "return chunks\n",
      "9: end function\n",
      "10:\n",
      "11: // Step 2: Text Embedding Generation\n",
      "12: function GetAdaEmbeddings(texts)\n",
      "13:\n",
      "Initialize OpenAI client with API key\n",
      "14:\n",
      "embeddings ←[], batch size ←2000\n",
      "15:\n",
      "for i = 0 to len(texts) step batch size do\n",
      "16:\n",
      "batch ←texts[i : i + batch size]\n",
      "17:\n",
      "response ←client.embeddings.create(model=”text-embedding-\n",
      "ada-002”, input=batch)\n",
      "18:\n",
      "batch embeddings ←extract embeddings from response\n",
      "19:\n",
      "embeddings.extend(batch embeddings)\n",
      "20:\n",
      "end for\n",
      "21:\n",
      "return np.array(embeddings) // Shape: (n samples, 1536)\n",
      "22: end function\n",
      "13\n",
      "\n",
      "Algorithm 2 Dimensionality Reduction and Emotional Intensity Estimation\n",
      "Input: High-dimensional embeddings from Step 2.\n",
      "Output: 3D embeddings and intensity scores\n",
      "1: // Step 3A: Dimensionality Reduction\n",
      "2: function FitTransformEmbeddings(embeddings)\n",
      "3:\n",
      "n components ←min(3, n samples, n features)\n",
      "4:\n",
      "Initialize StandardScaler() and PCA(n components)\n",
      "5:\n",
      "embeddings scaled ←scaler.fit transform(embeddings)\n",
      "6:\n",
      "embeddings 3d ←pca.fit transform(embeddings scaled)\n",
      "7:\n",
      "if embeddings 3d.shape[1] < 3 then\n",
      "8:\n",
      "Pad with zeros to ensure 3D representation\n",
      "9:\n",
      "end if\n",
      "10:\n",
      "return embeddings 3d\n",
      "11: end function\n",
      "12:\n",
      "13: // Step 3B: Emotional Intensity Estimation\n",
      "14: function EstimateEmotionIntensity(texts)\n",
      "15:\n",
      "Define word scores: Extreme (1.0), High (0.8), Moderate (0.6), Mild\n",
      "(0.3)\n",
      "16:\n",
      "intensities ←[]\n",
      "17:\n",
      "for each text in texts do\n",
      "18:\n",
      "intensity ←0.1, words ←extract words from text.lower()\n",
      "19:\n",
      "for each word in words do\n",
      "20:\n",
      "intensity ←intensity + word scores.get(word, 0)\n",
      "21:\n",
      "end for\n",
      "22:\n",
      "Apply modifiers: +0.3 (intensifiers), +0.2 (absolutists)\n",
      "23:\n",
      "intensity ←intensity + 0.25 × min(text.count(′!′), 4)\n",
      "24:\n",
      "intensity ←intensity + 0.15 × min(text.count(′?′), 3)\n",
      "25:\n",
      "if text.isupper() and len(text) > 3 then intensity ←intensity+\n",
      "0.5\n",
      "26:\n",
      "end if\n",
      "27:\n",
      "intensities.append(min(intensity, 2.0))\n",
      "28:\n",
      "end for\n",
      "29:\n",
      "return np.array(intensities)\n",
      "30: end function\n",
      "14\n",
      "\n",
      "Algorithm 3 Emotion Region Clustering and Brain Assignment\n",
      "Input: 3D embeddings and predefined brain regions\n",
      "Output: Brain region assignments and mappings\n",
      "1: // Step 4: Emotion Region Clustering\n",
      "2: function DefineEmotionRegions\n",
      "3:\n",
      "regions ←{25 brain regions with MNI coordinates}\n",
      "4:\n",
      "Examples: ’amygdala left’: [-20, -5, -18], ’insula right’: [40, 8, 0], ...\n",
      "5:\n",
      "return regions\n",
      "6: end function\n",
      "7: function PerformClustering(embeddings 3d, n regions = 25)\n",
      "8:\n",
      "n clusters ←min(n regions, embeddings 3d.shape[0])\n",
      "9:\n",
      "Initialize KMeans(n clusters, random state=42, n init=10)\n",
      "10:\n",
      "cluster centers ←kmeans.fit(embeddings 3d).cluster centers\n",
      "11:\n",
      "assignments\n",
      "←\n",
      "argmin(cdist(embeddings 3d,\n",
      "cluster centers),\n",
      "axis=1)\n",
      "12:\n",
      "return cluster centers, assignments\n",
      "13: end function\n",
      "14:\n",
      "15: // Step 5: Cluster-to-Region Assignment\n",
      "16: function\n",
      "AssignClustersToRegions(cluster centers,\n",
      "region coords)\n",
      "17:\n",
      "assigned regions ←[], used indices ←{}\n",
      "18:\n",
      "for each center in cluster centers do\n",
      "19:\n",
      "distances ←cdist([center], region coords)[0]\n",
      "20:\n",
      "for idx in argsort(distances) do\n",
      "21:\n",
      "if idx not in used indices then\n",
      "22:\n",
      "assigned regions.append(idx), used indices.add(idx)\n",
      "23:\n",
      "break\n",
      "24:\n",
      "end if\n",
      "25:\n",
      "end for\n",
      "26:\n",
      "end for\n",
      "27:\n",
      "return dict(zip(range(len(assigned regions)), assigned regions))\n",
      "28: end function\n",
      "15\n",
      "\n",
      "3. Results and Discussion\n",
      "Within this study, we considered each individually predicted regional brain\n",
      "engagement derived from textual emotional content analysis as a single acti-\n",
      "vation unit. Through the mapping of emotion-laden text clusters to anatom-\n",
      "ically defined brain regions, these activations represent computational infer-\n",
      "ences of regional involvement based on established emotion-brain relation-\n",
      "ships from neuroimaging literature [1–3, 50]. Each activation indicates the\n",
      "predicted engagement of specific neural structures that would theoretically\n",
      "be recruited during processing of the corresponding emotional content.\n",
      "3.1. Experiment 1: Healthy versus Depressed Subjects\n",
      "Emotion mapping results from the first experiment revealed notable dif-\n",
      "ferences in neural activity patterns between clinical interview transcripts\n",
      "of healthy individuals and those with depression (Figure 3). The analysis\n",
      "shows distinct activation profiles across different brain regions when com-\n",
      "paring healthy to depressed subjects. For the healthy individuals, robust\n",
      "emotional activations were observed across multiple brain regions, with par-\n",
      "ticularly strong responses in several key areas. The insula showed the high-\n",
      "est activation levels (reaching 7 units), followed by the isthmus region (also 7\n",
      "units), and the pericalcarine cortex (7 units). Other regions showing substan-\n",
      "tial activation included the amygdala (6 units), anterior cingulate (3 units),\n",
      "and hippocampus (6 units) [6]. This widespread activation pattern suggests\n",
      "healthy emotional processing involves coordinated activity across multiple\n",
      "brain networks, particularly within limbic-cortical circuits.\n",
      "In contrast, the depressed group demonstrated a markedly different activa-\n",
      "tion profile characterized by consistently lower activation levels across nearly\n",
      "all brain regions examined.\n",
      "Most regions in the depressed group showed\n",
      "activation levels between 1-3 units, representing substantial reductions com-\n",
      "pared to healthy controls. The most pronounced differences were observed\n",
      "in key emotional processing regions including the insula which dropped from\n",
      "7 to approximately 2 units, the isthmus (reduced from 7 to 2 units), and the\n",
      "pericalcarine cortex (reduced from 7 to 1 unit [51]).\n",
      "16\n",
      "\n",
      "Figure 3: Comparison of activation counts per brain region for healthy versus depressed\n",
      "subjects.\n",
      "The broader analysis by brain system categories (Figure 4) revealed system-\n",
      "atic differences in activation patterns. Cortical regions showed the most sub-\n",
      "stantial difference, with healthy subjects displaying approximately 40 total\n",
      "activations compared to about 13 in depressed subjects (a 67% reduction).\n",
      "Subcortical regions showed healthy subjects with 32 activations versus 14 in\n",
      "depressed subjects (a 56% reduction). The limbic system demonstrated the\n",
      "smallest absolute difference, with 23 activations in healthy subjects compared\n",
      "to 12 in depressed subjects, though this still represents a 48% reduction.\n",
      "The particularly pronounced reductions in cortical and subcortical activation\n",
      "suggest that depression affects both higher-order cognitive-emotional pro-\n",
      "cessing (cortical) and fundamental emotional response systems (subcortical).\n",
      "Large-scale comparative studies have found that gray matter volume reduc-\n",
      "tions in the insula and hippocampus represent common features across major\n",
      "psychiatric disorders, including depression [52–54]. Reduced hippocampal\n",
      "gray matter volume is a common feature of patients with major depression,\n",
      "bipolar disorder, and schizophrenia spectrum disorders [27].\n",
      "17\n",
      "\n",
      "Figure 4: Brain region analysis for healthy versus depressed subjects.\n",
      "Figure 5 presents a 3D cortical surface rendering in MNI space, displaying\n",
      "spatially localized differences in emotional processing between healthy and\n",
      "depressed subject groups (Table 2). The visualization employs a heat map\n",
      "approach where red regions indicate areas of greatest divergence in emotional\n",
      "activation patterns between the two populations, with intensity values rang-\n",
      "ing from 0.00 to 1.00 as shown in the color scale.\n",
      "The rendering reveals distinct anatomical clusters of emotional processing\n",
      "differences, with the most pronounced activation disparities concentrated in\n",
      "several key regions. Notable high-intensity areas (approaching the maximum\n",
      "1.00 value) are observed in bilateral insula regions, particularly prominent\n",
      "in the left hemisphere, along with posterior cingulate cortex involvement\n",
      "and brain stem areas corresponding to the raphe nuclei complex. Additional\n",
      "moderate-intensity differences are visible in frontal and temporal cortical re-\n",
      "gions. The left insula is associated with heightened perception of internal\n",
      "states, while raphe nuclei hyperactivity may indicate dysregulated seroton-\n",
      "ergic firing, leading to ineffective emotional modulation [55–57].\n",
      "18\n",
      "\n",
      "Figure 5: 3D rendering of the caudal (left) and parietal (right) regions with emotion\n",
      "intensity differences (Table 2) healthy and depressed subjects shown in red.\n",
      "Statistical significance tests were performed using the Mann-Whitney U test\n",
      "as detailed in Table 2, showing statistically significant differences for the\n",
      "insula left [58, 59] and raphe nuclei [60, 61] regions.\n",
      "19\n",
      "\n",
      "Table 2: Mann-Whitney U test between healthy and depressed group results.\n",
      "Region\n",
      "Healthy Mean\n",
      "Depressed Mean\n",
      "U Statistic\n",
      "p-value\n",
      "Significant\n",
      "Amygdala Left\n",
      "0.1000\n",
      "0.1000\n",
      "10.0000\n",
      "1.0000\n",
      "No\n",
      "Amygdala Right\n",
      "0.1000\n",
      "0.1000\n",
      "20.0000\n",
      "1.0000\n",
      "No\n",
      "Anterior Cingulate Right\n",
      "0.1000\n",
      "0.1000\n",
      "6.0000\n",
      "1.0000\n",
      "No\n",
      "Insula Left\n",
      "0.1000\n",
      "0.0781\n",
      "37.5000\n",
      "0.0041\n",
      "Yes\n",
      "Insula Right\n",
      "0.0950\n",
      "0.1000\n",
      "12.0000\n",
      "0.5002\n",
      "No\n",
      "Orbitofrontal Left\n",
      "0.1000\n",
      "0.1000\n",
      "4.5000\n",
      "1.0000\n",
      "No\n",
      "Hippocampus Left\n",
      "0.1000\n",
      "0.1000\n",
      "6.0000\n",
      "1.0000\n",
      "No\n",
      "Temporal Pole Left\n",
      "0.1000\n",
      "0.1000\n",
      "9.0000\n",
      "1.0000\n",
      "No\n",
      "Temporal Pole Right\n",
      "0.0950\n",
      "0.1000\n",
      "10.0000\n",
      "0.4237\n",
      "No\n",
      "Superior Temporal Left\n",
      "0.1000\n",
      "0.1000\n",
      "3.0000\n",
      "1.0000\n",
      "No\n",
      "Superior Temporal Right\n",
      "0.1000\n",
      "0.1000\n",
      "2.0000\n",
      "1.0000\n",
      "No\n",
      "Caudate Right\n",
      "0.0917\n",
      "0.1000\n",
      "3.0000\n",
      "0.5050\n",
      "No\n",
      "Putamen Left\n",
      "0.1000\n",
      "0.1000\n",
      "6.0000\n",
      "1.0000\n",
      "No\n",
      "Putamen Right\n",
      "0.1000\n",
      "0.1000\n",
      "10.0000\n",
      "1.0000\n",
      "No\n",
      "Nucleus Accumbens Left\n",
      "0.1000\n",
      "0.1000\n",
      "2.0000\n",
      "1.0000\n",
      "No\n",
      "Raphe Nuclei\n",
      "0.1000\n",
      "0.0750\n",
      "16.0000\n",
      "0.0131\n",
      "Yes\n",
      "Posterior Cingulate\n",
      "0.1000\n",
      "0.1000\n",
      "3.0000\n",
      "1.0000\n",
      "No\n",
      "3.2. Experiment 2: Multiple Emotional States\n",
      "The emotion intensity analysis results revealed a hierarchy of affective expe-\n",
      "riences, with love emerging as the most intense emotion (0.709), followed by\n",
      "joy (0.593) and relief (0.560). Negative emotions like sadness (0.486), fear\n",
      "(0.412), and anger (0.390) occupy middle-intensity positions.\n",
      "This inten-\n",
      "sity hierarchy suggests that basic positive emotions tend to be experienced\n",
      "more intensely than negative ones, with love showing remarkably high ac-\n",
      "tivation (Figure 6). The data also indicates that socially-oriented emotions\n",
      "(love, gratitude, curiosity) and approach-motivated states (joy, excitement)\n",
      "generate stronger neural responses than avoidance-motivated emotions (fear,\n",
      "disgust) or complex cognitive emotions requiring more nuanced processing\n",
      "(Figure 7).\n",
      "20\n",
      "\n",
      "Figure 6: Emotion intensity hierarchy from high activation count (left) to lower activation\n",
      "count (right).\n",
      "Figure 7: Intensity by emotional valence.\n",
      "These findings align with established emotion research, particularly regarding\n",
      "the valence-arousal relationship [51]. Research defines emotional valence as\n",
      "the extent to which an emotion is positive or negative, while arousal refers to\n",
      "its intensity, the strength of the associated emotional state. The results sup-\n",
      "ports the general principle that negative words tend to have higher arousal\n",
      "21\n",
      "\n",
      "values and are perceived with higher intensity than positive words [9], while\n",
      "also showing positive emotions like love and joy to be at the top of the in-\n",
      "tensity scale.\n",
      "The high intensity of love is particularly well-supported by neuroimaging\n",
      "research. Meta-analyses have found that love recruits brain regions that me-\n",
      "diate motivation, emotion, social cognition, and self-representation, includ-\n",
      "ing the ventral tegmental area, caudate nucleus, anterior cingulate gyrus,\n",
      "and middle frontal gyrus [62]. Further studies showed that positive emo-\n",
      "tions connect the prefrontal cortex to the nucleus accumbens, while negative\n",
      "emotions connect the nucleus accumbens to the amygdala [27], suggesting\n",
      "different neural pathways that could explain intensity differences.\n",
      "The positioning of joy as the second-highest intensity emotion is consistent\n",
      "with neuroscience research showing that the left prefrontal cortex is particu-\n",
      "larly associated with positive emotions including joy, with increased activity\n",
      "in the left prefrontal cortex correlated with positive emotional states [15]. Re-\n",
      "search identifies positive emotions like happiness, interest, satisfaction, pride,\n",
      "and love as being generated by individuals in response to internal and exter-\n",
      "nal stimuli [6], supporting the results showing that these emotions cluster in\n",
      "the high-intensity range. The relatively low intensity of cognitive emotions\n",
      "aligns with research suggesting these require more complex processing [63],\n",
      "but the moderate intensity of fear (0.412) is somewhat lower than might be\n",
      "expected given fear’s evolutionary importance [5].\n",
      "3.3. Experiment 3: Human versus LLM Chat bot\n",
      "Comparing the results of human textual conversation analysis with the re-\n",
      "sponses generated by an LLM chat bot (Figure 8), the analysis revealed\n",
      "distinct activation pattern differences between human and LLM conversa-\n",
      "tional profiles. Human texts demonstrated relatively balanced engagement\n",
      "across regions, with moderate amygdala activation suggesting appropriate\n",
      "emotional regulation [64, 65], balanced prefrontal cortex activity indicating\n",
      "cognitive control [66, 67], and measured caudate activation reflecting normal\n",
      "reward processing [68, 69]. Additionally, humans showed moderate orbito-\n",
      "frontal activation consistent with healthy social cognition [70, 71].\n",
      "In contrast, the LLM-generated responses exhibited markedly different pat-\n",
      "terns, including heightened activation in several regions such as the supe-\n",
      "22\n",
      "\n",
      "rior temporal areas, caudate, and orbito-frontal regions, alongside notably\n",
      "reduced activation in areas like the putamen and ventral tegmental area.\n",
      "These patterns suggest different underlying computational processes between\n",
      "human language generation and LLM text production [10, 11].\n",
      "Figure 8: Human vs. LLM chat bot emotion activation count comparison.\n",
      "Our proposed approach shows promise in distinguishing human-authored text\n",
      "from LLM-generated content, supporting recent studies [28–31] that have\n",
      "demonstrated the potential of computational approaches in analyzing text\n",
      "23\n",
      "\n",
      "to predict and classify various characteristics.\n",
      "These results suggest that\n",
      "natural language embeddings may encode information beyond surface-level\n",
      "semantics that correlates with different processing patterns.\n",
      "Figure 9 shows a 3D cortical rendering in MNI space, with lateral (left)\n",
      "and medial (right) views indicating the magnitude of differential activation\n",
      "between human subjects and LLM chat bot responses (Table 3). The visu-\n",
      "alization represents computationally derived activation patterns, where em-\n",
      "beddings originally in 1536-dimensional space were reduced to three princi-\n",
      "pal components using PCA and spatially projected onto the cortical surface.\n",
      "Red shading indicates the magnitude of differential activity captured by the\n",
      "model, with distinct patterns evident between humans and the LLM across\n",
      "multiple cortical regions.\n",
      "Figure 9: 3D rendering of the caudal (left) and parietal (right) regions with emotion\n",
      "intensity differences (Table 3) between human subjects and an LLM chat bot shown in\n",
      "red.\n",
      "Statistical significance tests (Table 3) using the Mann-Whitney U test showed\n",
      "significant statistical differences between human-authored text and the sub-\n",
      "sequent LLM-generated responses.\n",
      "24\n",
      "\n",
      "Table 3: Mann-Whitney U test for statistically significant differences in emotion response\n",
      "activation between human and chat bot group results.\n",
      "Region\n",
      "Human Mean\n",
      "Chat bot Mean\n",
      "U Statistic\n",
      "p-value\n",
      "Significant\n",
      "Amygdala Left\n",
      "0.3027\n",
      "0.2563\n",
      "85770.5\n",
      "0.0154\n",
      "Yes\n",
      "Amygdala Right\n",
      "0.3114\n",
      "0.2047\n",
      "81751.0\n",
      "0.0001\n",
      "Yes\n",
      "Anterior Cingulate Left\n",
      "0.3385\n",
      "0.5938\n",
      "43113.0\n",
      "0.0001\n",
      "Yes\n",
      "Anterior Cingulate Right\n",
      "0.2588\n",
      "0.1204\n",
      "182507.0\n",
      "0.0001\n",
      "Yes\n",
      "Insula Left\n",
      "0.1825\n",
      "0.2668\n",
      "26966.5\n",
      "0.0001\n",
      "Yes\n",
      "Insula Right\n",
      "0.3236\n",
      "0.1371\n",
      "131640.5\n",
      "0.0001\n",
      "Yes\n",
      "Orbitofrontal Left\n",
      "0.4074\n",
      "0.2805\n",
      "99956.0\n",
      "0.0001\n",
      "Yes\n",
      "Orbitofrontal Right\n",
      "0.1761\n",
      "0.2825\n",
      "33757.0\n",
      "0.0001\n",
      "Yes\n",
      "Hippocampus Left\n",
      "0.3129\n",
      "0.3085\n",
      "101184.0\n",
      "0.0004\n",
      "Yes\n",
      "Hippocampus Right\n",
      "0.1855\n",
      "0.3724\n",
      "65936.5\n",
      "0.0001\n",
      "Yes\n",
      "Prefrontal Cortex Left\n",
      "0.1823\n",
      "0.6846\n",
      "12674.5\n",
      "0.0001\n",
      "Yes\n",
      "Prefrontal Cortex Right\n",
      "0.1781\n",
      "0.4598\n",
      "16839.0\n",
      "0.0001\n",
      "Yes\n",
      "Temporal Pole Left\n",
      "0.2686\n",
      "0.1453\n",
      "122482.5\n",
      "0.0001\n",
      "Yes\n",
      "Temporal Pole Right\n",
      "0.2785\n",
      "0.2922\n",
      "47292.5\n",
      "0.0001\n",
      "Yes\n",
      "Superior Temporal Left\n",
      "0.3130\n",
      "0.1240\n",
      "86434.5\n",
      "0.0001\n",
      "Yes\n",
      "Superior Temporal Right\n",
      "0.3648\n",
      "0.2493\n",
      "89242.5\n",
      "0.0001\n",
      "Yes\n",
      "Caudate Left\n",
      "0.2425\n",
      "0.1508\n",
      "67663.0\n",
      "0.0001\n",
      "Yes\n",
      "Caudate Right\n",
      "0.4373\n",
      "0.2392\n",
      "16469.5\n",
      "0.0001\n",
      "Yes\n",
      "Putamen Left\n",
      "0.2063\n",
      "0.4327\n",
      "68661.0\n",
      "0.0001\n",
      "Yes\n",
      "Putamen Right\n",
      "0.2187\n",
      "0.1139\n",
      "103474.0\n",
      "0.0001\n",
      "Yes\n",
      "Nucleus Accumbens Left\n",
      "0.1867\n",
      "0.1351\n",
      "190880.0\n",
      "0.0001\n",
      "Yes\n",
      "Nucleus Accumbens Right\n",
      "0.2072\n",
      "0.1886\n",
      "48552.5\n",
      "0.0004\n",
      "Yes\n",
      "Hypothalamus\n",
      "0.2885\n",
      "0.2684\n",
      "25680.5\n",
      "0.6085\n",
      "No\n",
      "Periaqueductal Gray\n",
      "0.2471\n",
      "0.2699\n",
      "53214.0\n",
      "0.0001\n",
      "Yes\n",
      "Ventral Tegmental Area\n",
      "0.3154\n",
      "0.3274\n",
      "65572.5\n",
      "0.00001\n",
      "Yes\n",
      "25\n",
      "\n",
      "4. Study Limitations\n",
      "Several important limitations must be acknowledged. Firstly, the mapping\n",
      "from text embeddings to brain regions represents a computational model\n",
      "rather than direct measurement of neural activity. Secondly, further empir-\n",
      "ical validation through direct comparison with neuroimaging data, such as\n",
      "fMRI, would be necessary to establish the neurobiological validity of these\n",
      "mappings. Although recent advances have shown that brain signals from\n",
      "fMRI and EEG can be decoded into coherent text [72–74], the inverse prob-\n",
      "lem of predicting potentially engaged brain regions during language pro-\n",
      "cessing based on text input remains largely unexplored and requires careful\n",
      "validation. Finally, the predefined regional coordinates, while based on neu-\n",
      "roimaging research, represent population averages that may not accurately\n",
      "reflect individual neuro-anatomy.\n",
      "5. Conclusion\n",
      "The proposed approach represents a significant advancement in our ability\n",
      "to study emotional processing through computational methods. By leverag-\n",
      "ing state-of-the-art natural language processing techniques and established\n",
      "neuro-anatomical knowledge, the proposed approach offers a scalable, ac-\n",
      "cessible alternative to traditional neuroimaging methods. Our proposed ap-\n",
      "proach’s ability to process natural language and map emotional content onto\n",
      "anatomically defined brain regions opens new possibilities for understanding\n",
      "individual differences in emotional processing, monitoring mental health at\n",
      "scale, and developing personalized interventions.\n",
      "While important limitations exist, the approach offers compelling advan-\n",
      "tages in terms of cost, accessibility, and ecological validity. Future research\n",
      "should focus on validating the approach against established neuroimaging\n",
      "methods, addressing methodological limitations and exploring novel applica-\n",
      "tions in clinical and research contexts. The integration of this computational\n",
      "approach with traditional neuro-scientific methods has the potential to ac-\n",
      "celerate our understanding of the neural basis of emotion and contribute to\n",
      "more effective treatments for emotional and psychiatric disorders. To en-\n",
      "courage further exploration and application of the proposed approach, the\n",
      "complete source code used in this study is publicly available on GitHub at:\n",
      "https://github.com/xalentis/EmotionBrainMapping.\n",
      "26\n",
      "\n",
      "References\n",
      "[1] K. A. Lindquist, T. D. Wager, H. Kober, E. Bliss-Moreau, L. F. Barrett,\n",
      "The brain basis of emotion: A meta-analytic review, Behavioral and\n",
      "Brain Sciences 35 (3) (2012) 121–143.\n",
      "[2] K. Vytal, S. Hamann, Neuroimaging support for discrete neural corre-\n",
      "lates of basic emotions: a voxel-based meta-analysis, Journal of Cogni-\n",
      "tive Neuroscience 22 (12) (2010) 2864–2885.\n",
      "[3] F. C. Murphy, I. Nimmo-Smith, A. D. Lawrence, Functional neu-\n",
      "roanatomy of emotions: a meta-analysis, Cognitive, Affective, & Be-\n",
      "havioral Neuroscience 3 (3) (2003) 207–233.\n",
      "[4] H. Saarim¨aki, E. Glerean, L. Nummenmaa, Discrete neural signatures\n",
      "of basic emotions, Social Cognitive and Affective Neuroscience 17 (1)\n",
      "(2022) 26–36.\n",
      "[5] M. L. Phillips, W. C. Drevets, S. L. Rauch, R. Lane, Understand-\n",
      "ing the neurobiology of emotion perception:\n",
      "implications for af-\n",
      "fective disorders, Neuropsychopharmacology 28 (4) (2003) 645–655.\n",
      "doi:10.1038/sj.npp.1300136.\n",
      "[6] D. Sliz, S. Hayley, Major depressive disorder and alterations in\n",
      "insular\n",
      "cortical\n",
      "activity:\n",
      "A\n",
      "review\n",
      "of\n",
      "current\n",
      "functional\n",
      "mag-\n",
      "netic imaging research, Frontiers in Human Neuroscience 6 (2012).\n",
      "doi:10.3389/fnhum.2012.00323.\n",
      "URL https://www.frontiersin.org/articles/10.3389/fnhum.2012.00323\n",
      "[7] H. M. Ibrahim, A. Kulikova, H. Ly, A. J. Rush, E. Sherwood Brown,\n",
      "Anterior cingulate cortex in individuals with depressive symptoms: A\n",
      "structural mri study, Psychiatry Research: Neuroimaging 319 (2022)\n",
      "111420. doi:https://doi.org/10.1016/j.pscychresns.2021.111420.\n",
      "URL https://www.sciencedirect.com/science/article/pii/S0925492721001724\n",
      "[8] W. C. Drevets, Neuroimaging and neuropathological studies of de-\n",
      "pression:\n",
      "implications for the cognitive-emotional features of mood\n",
      "disorders, Current Opinion in Neurobiology 11 (2) (2001) 240–249.\n",
      "doi:https://doi.org/10.1016/S0959-4388(00)00203-8.\n",
      "URL https://www.sciencedirect.com/science/article/pii/S0959438800002038\n",
      "27\n",
      "\n",
      "[9] R. S. Hastings, R. V. Parsey, M. A. Oquendo, V. Arango, J. J. Mann,\n",
      "Volumetric analysis of the prefrontal cortex, amygdala, and hippocam-\n",
      "pus in major depression, Neuropsychopharmacology 29 (2004) 952–959.\n",
      "doi:10.1038/sj.npp.1300371.\n",
      "URL https://doi.org/10.1038/sj.npp.1300371\n",
      "[10] C. Caucheteux, J.-R. King, Language models align with brain activity\n",
      "without fine-tuning, Proceedings of the National Academy of Sciences\n",
      "119 (46) (2022) e2202651119.\n",
      "[11] M. Toneva, L. Wehbe, Brain embeddings of natural language processing\n",
      "models, Nature Neuroscience 25 (3) (2022) 369–377.\n",
      "[12] M. Schrimpf, I. A. Blank, G. Tuckute, C. Kauf, E. Hosseini, N. Kan-\n",
      "wisher, J. B. Tenenbaum, E. Fedorenko, Artificial neural networks accu-\n",
      "rately predict language processing in the brain, Nature Communications\n",
      "12 (1) (2021) 1–13.\n",
      "[13] B. Tomasino, P. Brambilla, et al., Emotionlanguage integration in the\n",
      "brain: Evidence from fmri and affective semantics, Frontiers in Psychol-\n",
      "ogy 14 (2023) 1167505.\n",
      "[14] X. Chen, Y. Li, H. Zhang, Decoding narrative valence from semantic\n",
      "and neural representations, NeuroImage 271 (2023) 120001.\n",
      "[15] R. J. Davidson, What does the prefrontal cortex do in affect: perspec-\n",
      "tives on frontal eeg asymmetry research, Biological psychology 67 (1-2)\n",
      "(2004) 219–234. doi:10.1016/j.biopsycho.2004.03.008.\n",
      "[16] J. Zhou, R. Wang, K. Kim, Semantic embeddings from large language\n",
      "models reflect human brain responses to emotional narratives, Journal\n",
      "of Neuroscience Methods 372 (2022) 109509.\n",
      "[17] L. Xiao, F. Zhang, M. Liu, Unsupervised learning of emotional clusters\n",
      "from language and their neural correlates, Cognitive Neurodynamics 15\n",
      "(2021) 987–1002.\n",
      "[18] P. B. Fitzgerald, A. R. Laird, J. Maller, Z. J. Daskalakis, A metaana-\n",
      "lytic study of changes in brain activation in depression, Human Brain\n",
      "Mapping 29 (6) (2007) 683–695. doi:10.1002/hbm.20426.\n",
      "28\n",
      "\n",
      "[19] K. N. Ochsner, L. F. Barrett, The neural basis of cognitive emotion:\n",
      "insights from lesion studies, Trends in Cognitive Sciences 7 (12) (2003)\n",
      "511–516. doi:10.1016/j.tics.2003.09.010.\n",
      "[20] K. Hoemann, M. Gendron, L. F. Barrett, Naturalistic language data\n",
      "reveal patterns of affective brain activation, Trends in Cognitive Sciences\n",
      "26 (4) (2022) 329–343.\n",
      "[21] J. Sacher,\n",
      "J. Neumann,\n",
      "T. Fnfstck,\n",
      "A. Soliman,\n",
      "A. Villringer,\n",
      "M. L. Schroeter, Mapping the depressed brain:\n",
      "A meta-analysis\n",
      "of structural and functional alterations in major depressive dis-\n",
      "order,\n",
      "Journal\n",
      "of\n",
      "Affective\n",
      "Disorders\n",
      "140\n",
      "(2)\n",
      "(2012)\n",
      "142–148.\n",
      "doi:https://doi.org/10.1016/j.jad.2011.08.001.\n",
      "URL https://www.sciencedirect.com/science/article/pii/S0165032711004587\n",
      "[22] A. G. Huth, W. A. de Heer, T. L. Griffiths, F. E. Theunissen, J. L. Gal-\n",
      "lant, Natural speech reveals the semantic maps that tile human cerebral\n",
      "cortex, Nature 532 (7600) (2016) 453–458.\n",
      "[23] C. Caucheteux, J.-R. King, Brains and algorithms partially converge\n",
      "in natural language processing, Communications Biology 6 (1) (2023)\n",
      "1–10.\n",
      "[24] A. Goldstein, Z. Zada, B. R. Buchsbaum, et al., Shared computational\n",
      "principles for language processing in humans and deep language models,\n",
      "Nature neuroscience 25 (3) (2022) 369–380.\n",
      "[25] R. Schwartz, M. Toneva, L. Wehbe, Inducing brain-relevant bias in nat-\n",
      "ural language processing models, Advances in Neural Information Pro-\n",
      "cessing Systems 32 (2019).\n",
      "[26] S. Campbell, M. Marriott, C. Nahmias, G. M. MacQueen, Lower\n",
      "hippocampal volume in patients suffering from depression: A meta-\n",
      "analysis, American Journal of Psychiatry 161 (4) (2004) 598–607.\n",
      "doi:10.1176/appi.ajp.161.4.598.\n",
      "[27] K. Brosch, F. Stein, S. Schmitt, J.-K. Pfarr, K. G. Ringwald, F. Thomas-\n",
      "Odenthal, T. Meller, O. Steinstrter, L. Waltemate, H. Lemke, S. Mein-\n",
      "ert, A. Winter, F. Breuer, K. Thiel, D. Grotegerd, T. Hahn, A. Jansen,\n",
      "U. Dannlowski, A. Krug, I. Nenadi, T. Kircher, Reduced hippocampal\n",
      "29\n",
      "\n",
      "gray matter volume is a common feature of patients with major depres-\n",
      "sion, bipolar disorder, and schizophrenia spectrum disorders, Molecular\n",
      "Psychiatry 27 (10) (2022) 4234–4243. doi:10.1038/s41380-022-01687-4.\n",
      "[28] J. M. Liu, M. Gao, S. Sabour, Z. Chen, M. Huang, T. M. C. Lee,\n",
      "Enhanced large language models for effective screening of depression\n",
      "and anxiety (2025). doi:10.48550/ARXIV.2501.08769.\n",
      "[29] Z. Ge, N. Hu, D. Li, Y. Wang, S. Qi, Y. Xu, H. Shi, J. Zhang, A survey\n",
      "of large language models in mental health disorder detection on social\n",
      "media (2025). doi:10.48550/ARXIV.2504.02800.\n",
      "[30] G. Lorenzoni, P. E. Velmovitsky, P. Alencar, D. Cowan, Gpt-4\n",
      "on clinic depression assessment:\n",
      "An llm-based pilot study (2025).\n",
      "doi:10.48550/ARXIV.2501.00199.\n",
      "[31] Z. Zhong, Z. Wang, Intelligent depression prevention via llm-based\n",
      "dialogue analysis:\n",
      "Overcoming the limitations of scale-dependent\n",
      "diagnosis\n",
      "through\n",
      "precise\n",
      "emotional\n",
      "pattern\n",
      "recognition\n",
      "(2025).\n",
      "doi:10.48550/ARXIV.2504.16504.\n",
      "[32] N. Ramirez-Esparza, U. Pavalanathan, et al., Psychological language\n",
      "shifts in social media posts about covid-19 reflect pandemic-related men-\n",
      "tal health challenges, Scientific Reports 12 (1) (2022) 1–14.\n",
      "[33] H. A. Schwartz, J. C. Eichstaedt, M. L. Kern, et al., Towards assessing\n",
      "changes in degree of depression through facebook, Proceedings of the\n",
      "Workshop on Computational Linguistics and Clinical Psychology: From\n",
      "Linguistic Signal to Clinical Reality (2014) 118–125.\n",
      "[34] Y. Zhou, et al., Depression detection via deep natural language process-\n",
      "ing: A systematic review, IEEE Access 9 (2021) 102578–102602.\n",
      "[35] A. Bulat, et al., Trustworthiness and risk in ai: A multidisciplinary\n",
      "perspective, Nature Machine Intelligence 5 (3) (2023) 190–205.\n",
      "[36] J. Gratch, R. Artstein, G. Lucas, G. Stratou, S. Scherer, A. Nazarian,\n",
      "R. Wood, J. Boberg, D. DeVault, S. Marsella, D. Traum, S. Rizzo, L.-P.\n",
      "Morency, The distress analysis interview corpus of human and com-\n",
      "puter interviews, in: N. Calzolari, K. Choukri, T. Declerck, H. Loftsson,\n",
      "30\n",
      "\n",
      "B. Maegaard, J. Mariani, A. Moreno, J. Odijk, S. Piperidis (Eds.), Pro-\n",
      "ceedings of the Ninth International Conference on Language Resources\n",
      "and Evaluation (LREC‘14), European Language Resources Association\n",
      "(ELRA), Reykjavik, Iceland, 2014, pp. 3123–3128.\n",
      "URL https://aclanthology.org/L14-1421/\n",
      "[37] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade,\n",
      "S. Ravi, Goemotions: A dataset of fine-grained emotions, in: Proceed-\n",
      "ings of the 58th Annual Meeting of the Association for Computational\n",
      "Linguistics (ACL), 2020, p. 1.\n",
      "URL https://arxiv.org/abs/2005.00547\n",
      "[38] Reddit users, Reddit comments and posts, https://www.reddit.com,\n",
      "data retrieved from Reddit for research purposes (n.d.).\n",
      "[39] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, P. Khaitan, Towards scal-\n",
      "able multi-domain conversational agents: The schema-guided dialogue\n",
      "dataset, in: Proceedings of the AAAI Conference on Artificial Intelli-\n",
      "gence, Vol. 34, 2020, pp. 8689–8696.\n",
      "[40] OpenAI, Gpt-4 technical report, https://openai.com/research/gpt-4,\n",
      "accessed: 2025-06-29 (2023).\n",
      "[41] S. Mohammad,\n",
      "F. Bravo-Marquez,\n",
      "M. Salameh,\n",
      "S. Kiritchenko,\n",
      "Semeval-2018 task 1: Affect in tweets, arXiv preprint arXiv:1704.06125\n",
      "(2018).\n",
      "[42] M. M. Bradley, P. J. Lang, Affective norms for english words (anew): In-\n",
      "struction manual and affective ratings, Technical report C-1, the center\n",
      "for research in psychophysiology, University of Florida (1999).\n",
      "[43] S. Baccianella, A. Esuli, F. Sebastiani, Sentiwordnet 3.0: an enhanced\n",
      "lexical resource for sentiment analysis and opinion mining, Lrec 10\n",
      "(2010) 2200–2204.\n",
      "[44] F. ˚A. Nielsen, A new anew: Evaluation of a word list for sentiment\n",
      "analysis in microblogs, arXiv preprint arXiv:1103.2903 (2011).\n",
      "[45] C. Hutto, E. Gilbert, Vader:\n",
      "A parsimonious rule-based model for\n",
      "sentiment analysis of social media text, in:\n",
      "Eighth International\n",
      "31\n",
      "\n",
      "Conference on Weblogs and Social Media (ICWSM-14), AAAI, 2014.\n",
      "URL https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109\n",
      "[46] S. Kiritchenko, X. Zhu, S. M. Mohammad, Sentiment analysis of short\n",
      "informal texts, Journal of Artificial Intelligence Research 50 (2014) 723–\n",
      "762.\n",
      "[47] H. Kober, L. F. Barrett, J. Joseph, E. Bliss-Moreau, K. Lindquist,\n",
      "T. D. Wager, Functional grouping and cortical–subcortical interactions\n",
      "in emotion: A meta-analysis of neuroimaging studies, NeuroImage 42 (2)\n",
      "(2008) 998–1031.\n",
      "[48] A. Etkin, T. Egner, R. Kalisch, Emotional processing in anterior cingu-\n",
      "late and medial prefrontal cortex, Trends in Cognitive Sciences 15 (2)\n",
      "(2011) 85–93.\n",
      "[49] W. W. Seeley, V. Menon, A. F. Schatzberg, J. Keller, G. H. Glover,\n",
      "H. Kenna, A. L. Reiss, M. D. Greicius, Dissociable intrinsic connectiv-\n",
      "ity networks for salience processing and executive control, Journal of\n",
      "Neuroscience 27 (9) (2007) 2349–2356.\n",
      "[50] K. L. Phan, T. Wager, S. F. Taylor, I. Liberzon, Functional neu-\n",
      "roanatomy of emotion: a meta-analysis of emotion activation studies\n",
      "in pet and fmri, NeuroImage 16 (2) (2002) 331–348.\n",
      "[51] W. C. Drevets, Neuroimaging studies of mood disorders, Biological Psy-\n",
      "chiatry 48 (8) (2000) 813–829.\n",
      "[52] M. Goodkind, S. B. Eickhoff, D. J. Oathes, Y. Jiang, A. Chang,\n",
      "L. B. Jones-Hagata, B. N. Ortega, Y. V. Zaiko, B. J. Roach, M. S.\n",
      "Korgaonkar, et al., Identification of a common neurobiological sub-\n",
      "strate for mental illness, JAMA Psychiatry 72 (4) (2015) 305–315.\n",
      "doi:10.1001/jamapsychiatry.2014.2206.\n",
      "[53] M. J. Kempton, R. Salvador, M. R. Munafo, J. R. Geddes, A. Sim-\n",
      "mons,\n",
      "S. Frangou,\n",
      "S. C. R. Williams,\n",
      "Meta-analysis,\n",
      "database,\n",
      "and meta-regression of 98 structural imaging studies in major de-\n",
      "pression,\n",
      "Archives of General Psychiatry 68 (7) (2011) 675–690.\n",
      "doi:10.1001/archgenpsychiatry.2011.60.\n",
      "32\n",
      "\n",
      "[54] L. Schmaal, D. J. Veltman, T. G. van Erp, P. G. Smann, T. Frodl, N. Ja-\n",
      "hanshad, E. Loehrer, H. Tiemeier, A. Hofman, W. J. Niessen, et al., Sub-\n",
      "cortical brain alterations in major depressive disorder: findings from the\n",
      "enigma major depressive disorder working group, Molecular Psychiatry\n",
      "21 (2016) 806–812. doi:10.1038/mp.2015.69.\n",
      "[55] C. Harshaw, Interoceptive dysfunction: Toward an integrated frame-\n",
      "work for understanding somatic and affective disturbance in depression,\n",
      "Psychological Bulletin 141 (2) (2015) 311–363.\n",
      "[56] J.-P. Hornung, The human raphe nuclei and the serotonergic system,\n",
      "Journal of Chemical Neuroanatomy 39 (2) (2010) 90–99.\n",
      "[57] M. P. Paulus, M. B. Stein, Interoception and anxiety: the importance\n",
      "of accurate perception of bodily signals, Biological Psychology 84 (1)\n",
      "(2010) 1–15.\n",
      "[58] D. Sliz, S. Hayley, Insula as a functional cortical hub in depression:\n",
      "Evidence from resting-state fmri studies, Brain Imaging and Behavior\n",
      "6 (2) (2012) 104–116.\n",
      "[59] A. Khundakar, A. Thomas, Structural and functional abnormalities in\n",
      "depression: the contribution of neuroimaging to the pathophysiology of\n",
      "major depressive disorder, Behavioural Pharmacology 20 (5-6) (2009)\n",
      "365–378.\n",
      "[60] B. Baumann, B. Bogerts, H. Bielau, The raphe nuclei and the serotoner-\n",
      "gic system in depression, Journal of Affective Disorders 98 (1-2) (2007)\n",
      "73–89.\n",
      "[61] J. Meyer, A. Wilson, N. Ginovart, V. Goulding, D. Hussey, K. Hood,\n",
      "S. Houle, Serotonin transporter binding potential in depressed subjects\n",
      "and healthy controls: A [11c] dasb pet imaging study, American Journal\n",
      "of Psychiatry 160 (3) (2003) 508–515.\n",
      "[62] L. Castanheira, C. Silva, E. Cheniaux, D. Telles-Correia, Neuroimaging\n",
      "correlates of depressionimplications to clinical practice, Frontiers in\n",
      "Psychiatry Volume 10 - 2019 (2019). doi:10.3389/fpsyt.2019.00703.\n",
      "URL https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.20\n",
      "33\n",
      "\n",
      "[63] K. N. Ochsner, L. Feldman Barrett, The neural basis of cognitive emo-\n",
      "tion: insights from lesion studies, Trends in Cognitive Sciences 7 (12)\n",
      "(2003) 511–516. doi:10.1016/j.tics.2003.09.010.\n",
      "[64] E. A. Phelps, Emotion and cognition: insights from studies of the human\n",
      "amygdala, Annual Review of Psychology 57 (2006) 27–53.\n",
      "[65] R. Adolphs, The biology of fear, Current Biology 23 (2) (2013) R79–R93.\n",
      "[66] E. K. Miller, J. D. Cohen, The prefrontal cortex and cognitive control,\n",
      "Nature Reviews Neuroscience 1 (1) (2000) 59–65.\n",
      "[67] E. Koechlin, C. Ody, F. Kouneiher, The architecture of cognitive control\n",
      "in the human prefrontal cortex, Science 302 (5648) (2003) 1181–1185.\n",
      "[68] W. Schultz, Neuronal reward and decision signals: from theories to data,\n",
      "Physiological Reviews 95 (3) (2015) 853–951.\n",
      "[69] R. C. O’Reilly, M. J. Frank, Making predictions in a changing world: the\n",
      "role of the basal ganglia in decision making, Frontiers in Neuroscience 7\n",
      "(2013) 109.\n",
      "[70] E. T. Rolls, The functions of the orbitofrontal cortex, Brain and Cogni-\n",
      "tion 55 (1) (2004) 11–29.\n",
      "[71] G. Schoenbaum, M. R. Roesch, T. A. Stalnaker, Y. K. Takahashi, A new\n",
      "perspective on the role of the orbitofrontal cortex in adaptive behaviour,\n",
      "Nature Reviews Neuroscience 10 (12) (2009) 885–892.\n",
      "[72] W. Qiu, Z. Huang, H. Hu, A. Feng, Y. Yan, R. Ying, Mindllm: A\n",
      "subject-agnostic and versatile model for fmri-to-text decoding, arXiv\n",
      "preprint arXiv:2502.15786 (2025).\n",
      "URL https://arxiv.org/abs/2502.15786\n",
      "[73] J. Tang, A. G. Huth, Semantic reconstruction of continuous language\n",
      "from non-invasive brain recordings, Nature Neuroscience 26 (2023) 873–\n",
      "880. doi:10.1038/s41593-023-01214-9.\n",
      "[74] J. L´evy, M. Zhang, S. Pinet, J. Rapin, H. Banville, S. d’Ascoli, J.-R.\n",
      "King, Brain-to-text decoding: A non-invasive approach via typing, arXiv\n",
      "preprint arXiv:2502.17480 (2025).\n",
      "URL https://arxiv.org/abs/2502.17480\n",
      "34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"C:\\\\Users\\\\ch939\\\\Downloads\\\\LLMBootCampCodes\\\\Week4\\\\data\\\\pdfs\\\\2508.09337v1.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Logic (Sliding Window):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embedding Generation (Sentence-Transformers):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store all chunks in a list with metadata (source file, index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "all_chunks = []\n",
    "for txt_file in os.listdir(\"data/texts\"):\n",
    "    text = open(f\"data/texts/{txt_file}\").read()\n",
    "    chunks = chunk_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": txt_file,\n",
    "            \"chunk_id\": i\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"chunks/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generate Embeddings **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ab2a927c7c460b9753d79ce5b34011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', use_auth_token=False)\n",
    "texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS requires float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build FAISS Index (optional) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void __cdecl faiss::write_index(const struct faiss::Index *,struct faiss::IOWriter *,int) at D:\\bld\\faiss-split_1734665116635\\work\\faiss\\impl\\index_write.cpp:858: don't know how to serialize this type of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9680\\1565754636.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Save index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"index/faiss_index.bin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ch939\\anaconda3\\envs\\rag_env\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m  12746\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12747\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in void __cdecl faiss::write_index(const struct faiss::Index *,struct faiss::IOWriter *,int) at D:\\bld\\faiss-split_1734665116635\\work\\faiss\\impl\\index_write.cpp:858: don't know how to serialize this type of index"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # or IndexIVFFlat for large scale\n",
    "\n",
    "# Use GPU (if available)\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(index, \"index/faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error occurs because you're trying to save a GPU index directly using faiss.write_index(), but Faiss does not support saving GPU indices directly. The write_index() function only works with CPU indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, you need to copy the index back to CPU before saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Use GPU (if available)\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings)\n",
    "\n",
    "# --- Move index back to CPU before saving ---\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    index = faiss.index_gpu_to_cpu(index)  # This converts GPU index back to CPU\n",
    "\n",
    "# Now save the index (only CPU indices can be saved)\n",
    "faiss.write_index(index, \"index/faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to load and use the index later (especially on GPU), do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load on CPU\n",
    "index = faiss.read_index(\"index/faiss_index.bin\")\n",
    "\n",
    "# Optionally move to GPU\n",
    "if faiss.get_num_gpus() > 0:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FAISS Indexing and Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [125.76947 126.49865 126.55414]\n",
      "Indices: [239 848 207]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "index.add(np.array(embeddings))  # add all chunk vectors\n",
    "\n",
    "# Example: search for a query embedding\n",
    "query_embedding = np.random.rand(1, dim).astype('float32')  # get embedding for the query (shape: [1, dim])\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "# indices[0] holds the top-k chunk indices\n",
    "print(\"Distances:\", distances[0])\n",
    "print(\"Indices:\", indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Query & Retrieve **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.611\n",
      "Text: abs/2505.02686. Xiaomi, L.-C.-T. 2025. MiMo-VL Technical Report. arXiv, abs/2506.03569. Xu, F.; Hao, Q.; Zong, Z.; Wang, J.; Zhang, Y.; Wang, J.; Lan, X.; Gong, J.; Ouyang, T.; Meng, F.; Shao, C.; Yan...\n",
      "\n",
      "Score: 0.589\n",
      "Text: Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianr...\n",
      "\n",
      "Score: 0.556\n",
      "Text: Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study Mahdi Dhaini \u0000[0000−0002−7831−3141], Juraj Vladika[0000−0002−4941−9166], Ege Erdogan[0000−0001−6170−...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How do large language models handle reasoning?\"\n",
    "query_embedding = model.encode([query])\n",
    "query_embedding = np.array(query_embedding).astype('float32')\n",
    "\n",
    "distances, indices = index.search(query_embedding, k=3)\n",
    "\n",
    "\"\"\"\n",
    "for i in indices[0]:\n",
    "    print(f\"Score: {1/(1+distances[0][i]):.3f}\")\n",
    "    print(f\"Text: {all_chunks[i]['text'][:200]}...\\n\")\n",
    "\"\"\"\n",
    "for i, d in zip(indices[0], distances[0]):\n",
    "    print(f\"Score: {1/(1+d):.3f}\")\n",
    "    print(f\"Text: {all_chunks[i]['text'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** FastAPI Endpoint **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a semantic search API built using FastAPI, leveraging sentence embeddings and FAISS (Facebook AI Similarity Search) for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load globally\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "index = faiss.read_index(\"index/faiss_index.bin\")\n",
    "with open(\"chunks/chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    query_vec = model.encode([q])\n",
    "    query_vec = np.array(query_vec).astype('float32')\n",
    "    D, I = index.search(query_vec, k=3)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        results.append({\n",
    "            \"score\": float(1 / (1 + D[0][0])),  # approximate similarity\n",
    "            \"text\": all_chunks[idx][\"text\"],\n",
    "            \"source\": all_chunks[idx][\"source\"]\n",
    "        })\n",
    "    return {\"query\": q, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FastAPI Route Skeleton:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is to set up a FastAPI endpoint that performs semantic search using a vector database (via FAISS) and a text embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    \"\"\"\n",
    "    Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "    \"\"\"\n",
    "    # TODO: Embed the query 'q' using your embedding model\n",
    "    query_vector =  model.encode([q])[0]  # Shape: [d_model]  # e.g., model.encode([q])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3\n",
    "    distances, indices = faiss_index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(chunks[idx])\n",
    "    return {\"query\": q, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Notebook / Script:** Complete code for the RAG pipeline (PDF extraction, chunking, embedding, indexing, retrieval).\n",
    "* **Data & Index:** The FAISS index file and the set of 50 processed paper chunks (e.g., as JSON or pickled objects).\n",
    "* **Retrieval Report:** A brief report showing at least 5 example queries and the top-3 retrieved passages for each, to demonstrate system performance.\n",
    "* **FastAPI Service:** The FastAPI app code (e.g. `main.py`) and instructions on how to run it. The `/search` endpoint should be demonstrable (e.g. returning top-3 passages in JSON for sample queries).\n",
    "\n",
    "## Student Exploration Tips\n",
    "\n",
    "* Experiment with different chunk sizes and overlaps. Smaller chunks (∼250 tokens) often give more precise retrieval, while larger chunks include more context.\n",
    "* Try different embedding models (e.g. using `'all-mpnet-base-v2'` or `'paraphrase-MiniLM-L6-v2'`) to see how retrieval results change.\n",
    "* Implement a simple reranking step: for example, after retrieving candidates with FAISS, re-score them with a cross-encoder model for finer ranking.\n",
    "* Use metadata: consider filtering or weighting chunks by paper metadata (e.g. year, authors, keywords) to improve relevance if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
